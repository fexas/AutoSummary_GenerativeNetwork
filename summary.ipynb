{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a006825",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3417a6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b5dfde",
   "metadata": {},
   "source": [
    "## lotka volterra and sir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9465ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = [[\"nn_prior\",\"bf\",\"dnnabc\",\"w2\"],[\"nn\",\"bf\",\"dnnabc\",\"w2\"]]\n",
    "benchmark = [\"lv\",\"sir\"]\n",
    "d_theta =[4,2]\n",
    "folder = [\"lotka_volterra\",\"SIR\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30c9ffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6a1befc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluation_metric  = [\"mean_bias\",\"refined_mean_bias\",\"median_bias\",\"refined_median_bias\"]\n",
    "\n",
    "# create a new pandas dataframe with the model and according evaluation metric\n",
    "df = pd.DataFrame(columns=[\"model\",\"benchmark\"] + evaluation_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8e9a578f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing nn_prior for lv...\n",
      "Processing bf for lv...\n",
      "Processing dnnabc for lv...\n",
      "Processing w2 for lv...\n",
      "Processing nn for sir...\n",
      "Processing bf for sir...\n",
      "Processing dnnabc for sir...\n",
      "Processing w2 for sir...\n"
     ]
    }
   ],
   "source": [
    "for k in range(2):\n",
    "    model_name_list = model[k]\n",
    "    benchmark_name = benchmark[k]\n",
    "    folder_name = folder[k]\n",
    "\n",
    "    for model_name in model_name_list:\n",
    "        result_path = os.path.join(os.getcwd(),f\"{folder_name}\" ,\"result\", f\"{model_name}_{benchmark_name}_result1.csv\")\n",
    "\n",
    "        if os.path.exists(result_path):\n",
    "            print(f\"Processing {model_name} for {benchmark_name}...\")\n",
    "            result_df = pd.read_csv(result_path)\n",
    "\n",
    "            mean_bias = result_df[\"bias\"].mean()\n",
    "            median_bias = result_df[\"bias\"].median()\n",
    "\n",
    "            if benchmark_name == \"lv\" and (model_name == \"nn_prior\" or model_name == \"bf\"):\n",
    "                refined_mean_bias = result_df[\"refined_bias\"].mean()\n",
    "                refined_median_bias = result_df[\"refined_bias\"].median()\n",
    "                if pd.isna(mean_bias) or pd.isna(refined_mean_bias) or pd.isna(median_bias) or pd.isna(refined_median_bias):\n",
    "                    print(\"NaN found in the result\")\n",
    "                    continue\n",
    "            elif benchmark_name == \"lv\" and (model_name == \"dnnabc\" or model_name == \"w2\"):\n",
    "                refined_mean_bias = np.nan\n",
    "                refined_median_bias = np.nan\n",
    "                if pd.isna(mean_bias)  or pd.isna(median_bias):\n",
    "                    print(\"NaN found in the result\")\n",
    "                    continue\n",
    "            elif benchmark_name == \"sir\" and (model_name == \"nn\" or model_name == \"bf\"):\n",
    "                refined_mean_bias = result_df[\"refined_bias\"].mean()\n",
    "                refined_median_bias = result_df[\"refined_bias\"].median()\n",
    "                if pd.isna(mean_bias) or pd.isna(refined_mean_bias) or pd.isna(median_bias) or pd.isna(refined_median_bias):\n",
    "                    print(\"NaN found in the result\")\n",
    "                    continue\n",
    "            elif benchmark_name == \"sir\" and (model_name == \"dnnabc\" or model_name == \"w2\"):\n",
    "                refined_mean_bias = np.nan\n",
    "                refined_median_bias = np.nan\n",
    "                if pd.isna(mean_bias)  or pd.isna(median_bias):\n",
    "                    print(\"NaN found in the result\")\n",
    "                    continue\n",
    "            \n",
    "            #  create a new row in the dataframe with the model name, benchmark name, and evaluation metrics\n",
    "            row = {\n",
    "                \"model\": model_name,\n",
    "                \"benchmark\": benchmark_name,\n",
    "                \"mean_bias\": mean_bias,\n",
    "                \"refined_mean_bias\": refined_mean_bias,\n",
    "                \"median_bias\": median_bias,\n",
    "                \"refined_median_bias\": refined_median_bias\n",
    "            }\n",
    "\n",
    "            df.loc[len(df)] = row\n",
    "\n",
    "        # result_df[\"model\"] = model_name\n",
    "        # result_df[\"benchmark\"] = benchmark_name\n",
    "        # df = pd.concat([df, result_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6972f58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      model benchmark  mean_bias  refined_mean_bias  median_bias  \\\n",
      "0  nn_prior        lv   0.146893           0.018062     0.147788   \n",
      "1        bf        lv   0.166159           0.020158     0.160045   \n",
      "2    dnnabc        lv   0.661575                NaN     0.668381   \n",
      "3        w2        lv   2.450806                NaN     2.458773   \n",
      "4        nn       sir   0.074626           0.001899     0.080167   \n",
      "5        bf       sir   0.004114           0.003209     0.003668   \n",
      "6    dnnabc       sir   0.010165                NaN     0.008890   \n",
      "7        w2       sir   0.045396                NaN     0.042235   \n",
      "\n",
      "   refined_median_bias  \n",
      "0             0.009961  \n",
      "1             0.008621  \n",
      "2                  NaN  \n",
      "3                  NaN  \n",
      "4             0.001543  \n",
      "5             0.003077  \n",
      "6                  NaN  \n",
      "7                  NaN  \n"
     ]
    }
   ],
   "source": [
    "# present df with pandas\n",
    "print(df)\n",
    "# save df to csv\n",
    "df.to_csv(os.path.join(os.getcwd(), \"summary.csv\"), index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9f2d41",
   "metadata": {},
   "source": [
    "## Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b2c7e428",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list =[\"nn\",\"bf\",\"dnnabc\",\"w2\"]\n",
    "benchmark_name = \"toy_example\"\n",
    "evaluation_metric = [\"mean_mmd\",\"refined_mean_mmd\",\"median_mmd\",\"refined_median_mmd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a28f7adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding nn for toy_example exists...\n",
      "mean_mmd: 0.0342724009, refined_mean_mmd: 0.013908406330000001, median_mmd: 0.0304384975, refined_median_mmd: 0.0126022475\n",
      "adding bf for toy_example exists...\n",
      "mean_mmd: 0.024483591300000003, refined_mean_mmd: 0.0170675768, median_mmd: 0.026694830500000002, refined_median_mmd: 0.015023101\n",
      "adding dnnabc for toy_example exists...\n",
      "mean_mmd: 0.132373988, refined_mean_mmd: nan, median_mmd: 0.13225222, refined_median_mmd: nan\n",
      "adding w2 for toy_example exists...\n",
      "mean_mmd: 0.130966615, refined_mean_mmd: nan, median_mmd: 0.130986445, refined_median_mmd: nan\n"
     ]
    }
   ],
   "source": [
    "toy_df = pd.DataFrame(columns=[\"model\",\"benchmark\"] + evaluation_metric)\n",
    "for model_name in model_list:\n",
    "    if model_name == \"nn\" or model_name == \"bf\":\n",
    "        result_path = os.path.join(os.getcwd(),f\"Toy_Example\" ,\"replicate_results\", f\"{model_name}_replicate_mmd.csv\")\n",
    "        if os.path.exists(result_path):\n",
    "            print(f\"adding {model_name} for {benchmark_name} exists...\")\n",
    "            result_df = pd.read_csv(result_path)\n",
    "            mean_mmd = result_df[\"MMD\"].mean()\n",
    "            median_mmd = result_df[\"MMD\"].median()\n",
    "            refined_mean_mmd = result_df[\"refined MMD\"].mean()\n",
    "            refined_median_mmd = result_df[\"refined MMD\"].median()\n",
    "            if pd.isna(mean_mmd) or pd.isna(refined_mean_mmd) or pd.isna(median_mmd) or pd.isna(refined_median_mmd):\n",
    "                print(\"NaN found in the result\")\n",
    "                continue\n",
    "            print(f\"mean_mmd: {mean_mmd}, refined_mean_mmd: {refined_mean_mmd}, median_mmd: {median_mmd}, refined_median_mmd: {refined_median_mmd}\")\n",
    "    else:\n",
    "        result_path = os.path.join(os.getcwd(),f\"Toy_Example\" ,\"result\", f\"{model_name}_50_result1.csv\")\n",
    "        if os.path.exists(result_path):\n",
    "            print(f\"adding {model_name} for {benchmark_name} exists...\")\n",
    "            result_df = pd.read_csv(result_path)\n",
    "            mean_mmd = result_df[\"mmd\"].mean()\n",
    "            median_mmd = result_df[\"mmd\"].median()\n",
    "            refined_mean_mmd = np.nan\n",
    "            refined_median_mmd = np.nan\n",
    "            if pd.isna(mean_mmd) or pd.isna(median_mmd):\n",
    "                print(\"NaN found in the result\")\n",
    "                continue\n",
    "            print(f\"mean_mmd: {mean_mmd}, refined_mean_mmd: {refined_mean_mmd}, median_mmd: {median_mmd}, refined_median_mmd: {refined_median_mmd}\")\n",
    "    \n",
    "    row = {\n",
    "        \"model\": model_name,\n",
    "        \"benchmark\": benchmark_name,\n",
    "        \"mean_mmd\": mean_mmd,\n",
    "        \"refined_mean_mmd\": refined_mean_mmd,\n",
    "        \"median_mmd\": median_mmd,\n",
    "        \"refined_median_mmd\": refined_median_mmd\n",
    "    }\n",
    "    toy_df.loc[len(toy_df)] = row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2e1eb24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    model    benchmark  mean_mmd  refined_mean_mmd  median_mmd  \\\n",
      "0      nn  toy_example  0.034272          0.013908    0.030438   \n",
      "1      bf  toy_example  0.024484          0.017068    0.026695   \n",
      "2  dnnabc  toy_example  0.132374               NaN    0.132252   \n",
      "3      w2  toy_example  0.130967               NaN    0.130986   \n",
      "\n",
      "   refined_median_mmd  \n",
      "0            0.012602  \n",
      "1            0.015023  \n",
      "2                 NaN  \n",
      "3                 NaN  \n"
     ]
    }
   ],
   "source": [
    "print(toy_df)\n",
    "# save toy_df to csv\n",
    "toy_df.to_csv(os.path.join(os.getcwd(), \"toy_summary.csv\"), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f1e418",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math5470",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
